{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":667,"status":"ok","timestamp":1766828209136,"user":{"displayName":"ThÃ¡i HÆ°ng","userId":"17420710039756366816"},"user_tz":-420},"id":"Agm__wgekIfx"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import requests\n","import os"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uFLhL5eoGF1j"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21553,"status":"ok","timestamp":1766828238723,"user":{"displayName":"ThÃ¡i HÆ°ng","userId":"17420710039756366816"},"user_tz":-420},"id":"wf06vXIcGUCh","outputId":"12ab914f-11bc-4820-ea6c-81eba7b18ea4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":28,"status":"ok","timestamp":1766828243439,"user":{"displayName":"ThÃ¡i HÆ°ng","userId":"17420710039756366816"},"user_tz":-420},"id":"81Y-sEapR3IH"},"outputs":[],"source":["os.makedirs('data', exist_ok=True)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":139,"status":"ok","timestamp":1766828245650,"user":{"displayName":"ThÃ¡i HÆ°ng","userId":"17420710039756366816"},"user_tz":-420},"id":"GuF70oW5R-Zc","outputId":"a2125d93-e13e-409a-fa85-7681cac9fb4e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Writing usgs_earthquake_preprocessing.py\n"]}],"source":["%%writefile usgs_earthquake_preprocessing.py\n","\"\"\"\n","USGS Earthquake Data Preprocessing Script\n","\"\"\"\n","\n","import pandas as pd\n","import numpy as np\n","import requests\n","from datetime import datetime\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","class USGSEarthquakeProcessor:\n","    \"\"\"\n","    Class to process earthquake data from USGS\n","    \"\"\"\n","\n","    def __init__(self, data_source=None):\n","        self.data_source = data_source\n","        self.raw_data = None\n","        self.processed_data = None\n","\n","    def load_data_from_file(self, file_path):\n","        \"\"\"Load data from CSV file\"\"\"\n","        print(f\"Reading file: {file_path}\")\n","\n","        try:\n","            self.raw_data = pd.read_csv(file_path)\n","            print(f\"Successfully loaded {len(self.raw_data)} records\")\n","        except Exception as e:\n","            print(f\"Error loading file: {str(e)}\")\n","\n","    def explore_data(self):\n","        \"\"\"Explore initial data\"\"\"\n","        if self.raw_data is None:\n","            print(\"No data loaded yet\")\n","            return\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"DATA INFORMATION\")\n","        print(\"=\"*80)\n","\n","        print(f\"\\n Data shape: {self.raw_data.shape}\")\n","        print(f\"   - Rows: {self.raw_data.shape[0]}\")\n","        print(f\"   - Columns: {self.raw_data.shape[1]}\")\n","\n","        print(f\"\\n Missing values:\")\n","        missing = self.raw_data.isnull().sum()\n","        missing_percent = (missing / len(self.raw_data) * 100).round(2)\n","\n","        missing_df = pd.DataFrame({\n","            'Column': missing.index,\n","            'Missing Count': missing.values,\n","            'Missing %': missing_percent.values\n","        })\n","        missing_df = missing_df[missing_df['Missing Count'] \u003e 0].sort_values('Missing %', ascending=False)\n","\n","        if len(missing_df) \u003e 0:\n","            print(missing_df.to_string(index=False))\n","        else:\n","            print(\"    No missing values\")\n","\n","    def drop_columns(self, columns_to_drop=['dmin', 'magError', 'magNst']):\n","        \"\"\"Drop columns with too many missing values\"\"\"\n","        if self.raw_data is None:\n","            print(\" No data to process\")\n","            return\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"  DROPPING COLUMNS\")\n","        print(\"=\"*80)\n","\n","        existing_cols = [col for col in columns_to_drop if col in self.raw_data.columns]\n","\n","        if existing_cols:\n","            print(f\"\\n Dropping columns: {existing_cols}\")\n","\n","            for col in existing_cols:\n","                missing_pct = (self.raw_data[col].isnull().sum() / len(self.raw_data) * 100).round(2)\n","                print(f\"   - {col}: {missing_pct}% missing\")\n","\n","            self.raw_data = self.raw_data.drop(columns=existing_cols)\n","            print(f\"\\n Dropped {len(existing_cols)} columns\")\n","\n","    def impute_missing_values(self, columns_to_impute=['nst', 'gap', 'horizontalError'], method='mean'):\n","        \"\"\"Impute missing values with mean\"\"\"\n","        if self.raw_data is None:\n","            print(\" No data to process\")\n","            return\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(\" IMPUTING MISSING VALUES\")\n","        print(\"=\"*80)\n","\n","        for col in columns_to_impute:\n","            if col in self.raw_data.columns:\n","                missing_before = self.raw_data[col].isnull().sum()\n","\n","                if missing_before \u003e 0:\n","                    if method == 'mean':\n","                        fill_value = self.raw_data[col].mean()\n","                    elif method == 'median':\n","                        fill_value = self.raw_data[col].median()\n","                    else:\n","                        fill_value = self.raw_data[col].mode()[0]\n","\n","                    self.raw_data[col].fillna(fill_value, inplace=True)\n","\n","                    print(f\"\\n {col}:\")\n","                    print(f\"   - Missing: {missing_before}\")\n","                    print(f\"   - Filled with {method}: {fill_value:.4f}\")\n","\n","    def convert_time_format(self, time_column='time'):\n","        \"\"\"Convert time format to datetime\"\"\"\n","        if self.raw_data is None:\n","            print(\"No data to process\")\n","            return\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(\" CONVERTING TIME FORMAT\")\n","        print(\"=\"*80)\n","\n","        if time_column not in self.raw_data.columns:\n","            print(f\" Column '{time_column}' not found\")\n","            return\n","\n","        try:\n","            print(f\"\\n Converting column '{time_column}' to datetime...\")\n","\n","            self.raw_data[time_column] = pd.to_datetime(self.raw_data[time_column], utc=True)\n","\n","            self.raw_data['year'] = self.raw_data[time_column].dt.year\n","            self.raw_data['month'] = self.raw_data[time_column].dt.month\n","            self.raw_data['day'] = self.raw_data[time_column].dt.day\n","            self.raw_data['hour'] = self.raw_data[time_column].dt.hour\n","            self.raw_data['dayofweek'] = self.raw_data[time_column].dt.dayofweek\n","            self.raw_data['dayofyear'] = self.raw_data[time_column].dt.dayofyear\n","\n","            print(f\" Successfully converted!\")\n","            print(f\"\\n Time range:\")\n","            print(f\"   - From: {self.raw_data[time_column].min()}\")\n","            print(f\"   - To: {self.raw_data[time_column].max()}\")\n","\n","        except Exception as e:\n","            print(f\" Error converting time: {str(e)}\")\n","\n","    def feature_engineering(self):\n","        \"\"\"Create additional features\"\"\"\n","        if self.raw_data is None:\n","            print(\" No data to process\")\n","            return\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"  FEATURE ENGINEERING\")\n","        print(\"=\"*80)\n","\n","        if 'mag' in self.raw_data.columns:\n","            print(\"\\n Creating energy column from magnitude...\")\n","            self.raw_data['energy'] = 10 ** (1.5 * self.raw_data['mag'] + 4.8)\n","            print(\" Created 'energy' column\")\n","\n","        if 'mag' in self.raw_data.columns:\n","            print(\"\\n Categorizing earthquakes by magnitude...\")\n","\n","            def categorize_magnitude(mag):\n","                if pd.isna(mag):\n","                    return 'Unknown'\n","                elif mag \u003c 5.0:\n","                    return 'Small'\n","                elif 5.0 \u003c= mag \u003c 6.0:\n","                    return 'Moderate'\n","                elif 6.0 \u003c= mag \u003c 7.0:\n","                    return 'Strong'\n","                elif 7.0 \u003c= mag \u003c 8.0:\n","                    return 'Major'\n","                else:\n","                    return 'Great'\n","\n","            self.raw_data['magnitude_category'] = self.raw_data['mag'].apply(categorize_magnitude)\n","            print(\" Created 'magnitude_category' column\")\n","\n","        if 'depth' in self.raw_data.columns:\n","            print(\"\\n Categorizing earthquake depth...\")\n","\n","            def categorize_depth(depth):\n","                if pd.isna(depth):\n","                    return 'Unknown'\n","                elif depth \u003c 70:\n","                    return 'Shallow'\n","                elif 70 \u003c= depth \u003c 300:\n","                    return 'Intermediate'\n","                else:\n","                    return 'Deep'\n","\n","            self.raw_data['depth_category'] = self.raw_data['depth'].apply(categorize_depth)\n","            print(\" Created 'depth_category' column\")\n","\n","    def remove_duplicates(self):\n","        \"\"\"Remove duplicate records\"\"\"\n","        if self.raw_data is None:\n","            print(\" No data to process\")\n","            return\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(\" REMOVING DUPLICATES\")\n","        print(\"=\"*80)\n","\n","        initial_count = len(self.raw_data)\n","        self.raw_data = self.raw_data.drop_duplicates()\n","        final_count = len(self.raw_data)\n","        duplicates_removed = initial_count - final_count\n","\n","        print(f\"\\n Results:\")\n","        print(f\"   - Initial records: {initial_count}\")\n","        print(f\"   - Final records: {final_count}\")\n","        print(f\"   - Duplicates removed: {duplicates_removed}\")\n","\n","    def standardize_columns(self):\n","        \"\"\"Standardize column names\"\"\"\n","        if self.raw_data is None:\n","            print(\" No data to process\")\n","            return\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(\" STANDARDIZING COLUMN NAMES\")\n","        print(\"=\"*80)\n","\n","        column_mapping = {\n","            'time': 'Timestamp',\n","            'mag': 'Magnitude',\n","            'latitude': 'Latitude',\n","            'longitude': 'Longitude',\n","            'depth': 'Depth'\n","        }\n","\n","        existing_mapping = {k: v for k, v in column_mapping.items() if k in self.raw_data.columns}\n","\n","        if existing_mapping:\n","            self.raw_data = self.raw_data.rename(columns=existing_mapping)\n","            print(\" Standardized column names:\")\n","            for old, new in existing_mapping.items():\n","                print(f\"   - {old} â†’ {new}\")\n","\n","    def save_processed_data(self, output_path='processed_earthquake_data.csv'):\n","        \"\"\"Save processed data\"\"\"\n","        if self.raw_data is None:\n","            print(\" No data to save\")\n","            return\n","\n","        print(\"\\n\" + \"=\"*80)\n","        print(\"SAVING DATA\")\n","        print(\"=\"*80)\n","\n","        try:\n","            self.processed_data = self.raw_data.copy()\n","            self.processed_data.to_csv(output_path, index=False)\n","\n","            print(f\"\\n Data saved successfully!\")\n","            print(f\" File: {output_path}\")\n","            print(f\" Records: {len(self.processed_data)}\")\n","            print(f\" Columns: {len(self.processed_data.columns)}\")\n","\n","        except Exception as e:\n","            print(f\" Error saving file: {str(e)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":182},"id":"LEE4RgVWTqEo"},"outputs":[{"name":"stdout","output_type":"stream","text":["Please select your earthquake data files...\n","Select all 4 CSV files:\n","  - usgs_earthquake_data_2002_2011.csv\n","  - usgs_earthquake_data_2011_2013.csv\n","  - usgs_earthquake_data_2013_2015.csv\n","  - usgs_earthquake_data_2015_2025.csv\n","\n","Click 'Choose Files' button below \n"]},{"data":{"text/html":["\n","     \u003cinput type=\"file\" id=\"files-e8d6579b-00aa-4bc9-930a-b2565315290a\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" /\u003e\n","     \u003coutput id=\"result-e8d6579b-00aa-4bc9-930a-b2565315290a\"\u003e\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      \u003c/output\u003e\n","      \u003cscript\u003e// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) =\u003e {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable\u003c!Object\u003e} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) =\u003e {\n","    inputElement.addEventListener('change', (e) =\u003e {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) =\u003e {\n","    cancel.onclick = () =\u003e {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) =\u003e {\n","      const reader = new FileReader();\n","      reader.onload = (e) =\u003e {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position \u003c fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","\u003c/script\u003e "],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"ename":"TypeError","evalue":"'NoneType' object is not subscriptable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-985197181.py\u001b[0m in \u001b[0;36m\u003ccell line: 0\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nClick 'Choose Files' button below \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 11\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Move files to data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    169\u001b[0m   \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_collections\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 171\u001b[0;31m   \u001b[0;32mwhile\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'complete'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m     result = _output.eval_js(\n\u001b[1;32m    173\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n","\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"]}],"source":["from google.colab import files\n","\n","print(\"Please select your earthquake data files...\")\n","print(\"Select all 4 CSV files:\")\n","print(\"  - usgs_earthquake_data_2002_2011.csv\")\n","print(\"  - usgs_earthquake_data_2011_2013.csv\")\n","print(\"  - usgs_earthquake_data_2013_2015.csv\")\n","print(\"  - usgs_earthquake_data_2015_2025.csv\")\n","print(\"\\nClick 'Choose Files' button below \")\n","\n","uploaded = files.upload()\n","\n","# Move files to data directory\n","for filename in uploaded.keys():\n","    os.rename(filename, f'data/{filename}')\n","    print(f\" Uploaded: {filename}\")\n","\n","print(\"\\n All files uploaded successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":106,"status":"ok","timestamp":1765203388524,"user":{"displayName":"Minh LuÃ¢n","userId":"15523114585941358677"},"user_tz":-420},"id":"3eezcGowTzum","outputId":"86555468-a4ef-4f0a-f8ed-4ee5a498275f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files in data directory:\n","total 547M\n","-rw-r--r-- 1 root root 181M Dec  8 14:07 usgs_earthquake_data_2002_2011.csv\n","-rw-r--r-- 1 root root  47M Dec  8 14:06 usgs_earthquake_data_2011_2013.csv\n","-rw-r--r-- 1 root root  48M Dec  8 14:06 usgs_earthquake_data_2013_2015.csv\n","-rw-r--r-- 1 root root 273M Dec  8 14:08 usgs_earthquake_data_2015_2025.csv\n"]}],"source":["print(\"Files in data directory:\")\n","!ls -lh data/"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43374,"status":"ok","timestamp":1765203432090,"user":{"displayName":"Minh LuÃ¢n","userId":"15523114585941358677"},"user_tz":-420},"id":"JSii_ZiRT8bh","outputId":"ef55ffe3-4bed-430d-d74d-d5ebe75bb702"},"outputs":[{"name":"stdout","output_type":"stream","text":["================================================================================\n","PROCESSING FILE: 2002-2011 DATA\n","================================================================================\n","Reading file: data/usgs_earthquake_data_2002_2011.csv\n","Successfully loaded 1026123 records\n","\n","================================================================================\n","  DROPPING COLUMNS\n","================================================================================\n","\n"," Dropping columns: ['dmin', 'magError', 'magNst']\n","   - dmin: 49.35% missing\n","   - magError: 59.96% missing\n","   - magNst: 41.48% missing\n","\n"," Dropped 3 columns\n","\n","================================================================================\n","ðŸ”§ IMPUTING MISSING VALUES\n","================================================================================\n","\n"," nst:\n","   - Missing: 227423\n","   - Filled with mean: 20.3557\n","\n"," gap:\n","   - Missing: 256403\n","   - Filled with mean: 136.1259\n","\n"," horizontalError:\n","   - Missing: 542422\n","   - Filled with mean: 0.7536\n","\n","================================================================================\n"," CONVERTING TIME FORMAT\n","================================================================================\n","\n"," Converting column 'time' to datetime...\n"," Successfully converted!\n","\n"," Time range:\n","   - From: 2002-01-01 00:25:29.220000+00:00\n","   - To: 2011-04-13 23:58:07.650000+00:00\n","\n","================================================================================\n","  FEATURE ENGINEERING\n","================================================================================\n","\n"," Creating energy column from magnitude...\n"," Created 'energy' column\n","\n"," Categorizing earthquakes by magnitude...\n"," Created 'magnitude_category' column\n","\n","ðŸŒŠ Categorizing earthquake depth...\n"," Created 'depth_category' column\n","\n","================================================================================\n"," REMOVING DUPLICATES\n","================================================================================\n","\n"," Results:\n","   - Initial records: 1026123\n","   - Final records: 1026123\n","   - Duplicates removed: 0\n","\n","================================================================================\n"," STANDARDIZING COLUMN NAMES\n","================================================================================\n"," Standardized column names:\n","   - time â†’ Timestamp\n","   - mag â†’ Magnitude\n","   - latitude â†’ Latitude\n","   - longitude â†’ Longitude\n","   - depth â†’ Depth\n","\n","================================================================================\n","SAVING DATA\n","================================================================================\n","\n"," Data saved successfully!\n"," File: processed_2002_2011.csv\n"," Records: 1026123\n"," Columns: 28\n","\n"," FILE COMPLETE!\n"]}],"source":["from usgs_earthquake_preprocessing import USGSEarthquakeProcessor\n","\n","print(\"=\"*80)\n","print(\"PROCESSING FILE: 2002-2011 DATA\")\n","print(\"=\"*80)\n","\n","# Create processor\n","processor1 = USGSEarthquakeProcessor()\n","\n","# Load data\n","processor1.load_data_from_file('data/usgs_earthquake_data_2002_2011.csv')\n","\n","# Process\n","processor1.drop_columns(['dmin', 'magError', 'magNst'])\n","processor1.impute_missing_values(['nst', 'gap', 'horizontalError'], method='mean')\n","processor1.convert_time_format('time')\n","processor1.feature_engineering()\n","processor1.remove_duplicates()\n","processor1.standardize_columns()\n","\n","# Save\n","processor1.save_processed_data('processed_2002_2011.csv')\n","\n","print(\"\\n FILE COMPLETE!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10585,"status":"ok","timestamp":1765203470669,"user":{"displayName":"Minh LuÃ¢n","userId":"15523114585941358677"},"user_tz":-420},"id":"xqd-q-x5UviW","outputId":"b1e4b9c2-5021-442d-b596-24fb3beb5942"},"outputs":[{"name":"stdout","output_type":"stream","text":["================================================================================\n","PROCESSING FILE 2/4: 2011-2013 DATA\n","================================================================================\n","Reading file: data/usgs_earthquake_data_2011_2013.csv\n","Successfully loaded 257916 records\n","\n","================================================================================\n","  DROPPING COLUMNS\n","================================================================================\n","\n"," Dropping columns: ['dmin', 'magError', 'magNst']\n","   - dmin: 39.41% missing\n","   - magError: 45.04% missing\n","   - magNst: 29.69% missing\n","\n"," Dropped 3 columns\n","\n","================================================================================\n","ðŸ”§ IMPUTING MISSING VALUES\n","================================================================================\n","\n"," nst:\n","   - Missing: 57623\n","   - Filled with mean: 25.3741\n","\n"," gap:\n","   - Missing: 57537\n","   - Filled with mean: 127.0790\n","\n"," horizontalError:\n","   - Missing: 122446\n","   - Filled with mean: 0.7590\n","\n","================================================================================\n"," CONVERTING TIME FORMAT\n","================================================================================\n","\n"," Converting column 'time' to datetime...\n"," Successfully converted!\n","\n"," Time range:\n","   - From: 2011-04-14 00:05:44.766000+00:00\n","   - To: 2013-08-30 23:58:39.575000+00:00\n","\n","================================================================================\n","  FEATURE ENGINEERING\n","================================================================================\n","\n"," Creating energy column from magnitude...\n"," Created 'energy' column\n","\n"," Categorizing earthquakes by magnitude...\n"," Created 'magnitude_category' column\n","\n","ðŸŒŠ Categorizing earthquake depth...\n"," Created 'depth_category' column\n","\n","================================================================================\n"," REMOVING DUPLICATES\n","================================================================================\n","\n"," Results:\n","   - Initial records: 257916\n","   - Final records: 257916\n","   - Duplicates removed: 0\n","\n","================================================================================\n"," STANDARDIZING COLUMN NAMES\n","================================================================================\n"," Standardized column names:\n","   - time â†’ Timestamp\n","   - mag â†’ Magnitude\n","   - latitude â†’ Latitude\n","   - longitude â†’ Longitude\n","   - depth â†’ Depth\n","\n","================================================================================\n","SAVING DATA\n","================================================================================\n","\n"," Data saved successfully!\n"," File: processed_2011_2013.csv\n"," Records: 257916\n"," Columns: 28\n","\n"," FILE 2/4 COMPLETE!\n"]}],"source":["from usgs_earthquake_preprocessing import USGSEarthquakeProcessor\n","\n","print(\"=\"*80)\n","print(\"PROCESSING FILE 2/4: 2011-2013 DATA\")\n","print(\"=\"*80)\n","\n","processor2 = USGSEarthquakeProcessor()\n","processor2.load_data_from_file('data/usgs_earthquake_data_2011_2013.csv')\n","processor2.drop_columns(['dmin', 'magError', 'magNst'])\n","processor2.impute_missing_values(['nst', 'gap', 'horizontalError'], method='mean')\n","processor2.convert_time_format('time')\n","processor2.feature_engineering()\n","processor2.remove_duplicates()\n","processor2.standardize_columns()\n","processor2.save_processed_data('processed_2011_2013.csv')\n","\n","print(\"\\n FILE 2/4 COMPLETE!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11173,"status":"ok","timestamp":1765203508612,"user":{"displayName":"Minh LuÃ¢n","userId":"15523114585941358677"},"user_tz":-420},"id":"a-bJ77eDU5gg","outputId":"0e5aad44-84f9-4970-fbb0-d2c5d6f5d806"},"outputs":[{"name":"stdout","output_type":"stream","text":["================================================================================\n","PROCESSING FILE 3/4: 2013-2015 DATA\n","================================================================================\n","Reading file: data/usgs_earthquake_data_2013_2015.csv\n","Successfully loaded 265725 records\n","\n","================================================================================\n","  DROPPING COLUMNS\n","================================================================================\n","\n"," Dropping columns: ['dmin', 'magError', 'magNst']\n","   - dmin: 33.16% missing\n","   - magError: 32.93% missing\n","   - magNst: 31.1% missing\n","\n"," Dropped 3 columns\n","\n","================================================================================\n","ðŸ”§ IMPUTING MISSING VALUES\n","================================================================================\n","\n"," nst:\n","   - Missing: 112354\n","   - Filled with mean: 16.5288\n","\n"," gap:\n","   - Missing: 75805\n","   - Filled with mean: 127.0553\n","\n"," horizontalError:\n","   - Missing: 114429\n","   - Filled with mean: 1.7898\n","\n","================================================================================\n"," CONVERTING TIME FORMAT\n","================================================================================\n","\n"," Converting column 'time' to datetime...\n"," Successfully converted!\n","\n"," Time range:\n","   - From: 2013-08-31 00:02:26.723000+00:00\n","   - To: 2015-08-28 23:48:40.730000+00:00\n","\n","================================================================================\n","  FEATURE ENGINEERING\n","================================================================================\n","\n"," Creating energy column from magnitude...\n"," Created 'energy' column\n","\n"," Categorizing earthquakes by magnitude...\n"," Created 'magnitude_category' column\n","\n","ðŸŒŠ Categorizing earthquake depth...\n"," Created 'depth_category' column\n","\n","================================================================================\n"," REMOVING DUPLICATES\n","================================================================================\n","\n"," Results:\n","   - Initial records: 265725\n","   - Final records: 265725\n","   - Duplicates removed: 0\n","\n","================================================================================\n"," STANDARDIZING COLUMN NAMES\n","================================================================================\n"," Standardized column names:\n","   - time â†’ Timestamp\n","   - mag â†’ Magnitude\n","   - latitude â†’ Latitude\n","   - longitude â†’ Longitude\n","   - depth â†’ Depth\n","\n","================================================================================\n","SAVING DATA\n","================================================================================\n","\n"," Data saved successfully!\n"," File: processed_2013_2015.csv\n"," Records: 265725\n"," Columns: 28\n","\n"," FILE 3/4 COMPLETE!\n"]}],"source":["from usgs_earthquake_preprocessing import USGSEarthquakeProcessor\n","\n","print(\"=\"*80)\n","print(\"PROCESSING FILE 3/4: 2013-2015 DATA\")\n","print(\"=\"*80)\n","\n","processor3 = USGSEarthquakeProcessor()\n","processor3.load_data_from_file('data/usgs_earthquake_data_2013_2015.csv')\n","processor3.drop_columns(['dmin', 'magError', 'magNst'])\n","processor3.impute_missing_values(['nst', 'gap', 'horizontalError'], method='mean')\n","processor3.convert_time_format('time')\n","processor3.feature_engineering()\n","processor3.remove_duplicates()\n","processor3.standardize_columns()\n","processor3.save_processed_data('processed_2013_2015.csv')\n","\n","print(\"\\n FILE 3/4 COMPLETE!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65377,"status":"ok","timestamp":1765203584888,"user":{"displayName":"Minh LuÃ¢n","userId":"15523114585941358677"},"user_tz":-420},"id":"nieXk3U5U-ZT","outputId":"e54ef8f8-44cc-4e38-ca54-84bb5a0105ce"},"outputs":[{"name":"stdout","output_type":"stream","text":["================================================================================\n","PROCESSING FILE 4/4: 2015-2025 DATA\n","================================================================================\n","Reading file: data/usgs_earthquake_data_2015_2025.csv\n","Successfully loaded 1489681 records\n","\n","================================================================================\n","  DROPPING COLUMNS\n","================================================================================\n","\n"," Dropping columns: ['dmin', 'magError', 'magNst']\n","   - dmin: 31.95% missing\n","   - magError: 29.11% missing\n","   - magNst: 27.45% missing\n","\n"," Dropped 3 columns\n","\n","================================================================================\n","ðŸ”§ IMPUTING MISSING VALUES\n","================================================================================\n","\n"," nst:\n","   - Missing: 513785\n","   - Filled with mean: 21.1209\n","\n"," gap:\n","   - Missing: 389700\n","   - Filled with mean: 121.6653\n","\n"," horizontalError:\n","   - Missing: 522286\n","   - Filled with mean: 2.1258\n","\n","================================================================================\n"," CONVERTING TIME FORMAT\n","================================================================================\n","\n"," Converting column 'time' to datetime...\n"," Successfully converted!\n","\n"," Time range:\n","   - From: 2015-08-01 00:07:41.692000+00:00\n","   - To: 2025-12-06 09:40:47.601000+00:00\n","\n","================================================================================\n","  FEATURE ENGINEERING\n","================================================================================\n","\n"," Creating energy column from magnitude...\n"," Created 'energy' column\n","\n"," Categorizing earthquakes by magnitude...\n"," Created 'magnitude_category' column\n","\n","ðŸŒŠ Categorizing earthquake depth...\n"," Created 'depth_category' column\n","\n","================================================================================\n"," REMOVING DUPLICATES\n","================================================================================\n","\n"," Results:\n","   - Initial records: 1489681\n","   - Final records: 1489681\n","   - Duplicates removed: 0\n","\n","================================================================================\n"," STANDARDIZING COLUMN NAMES\n","================================================================================\n"," Standardized column names:\n","   - time â†’ Timestamp\n","   - mag â†’ Magnitude\n","   - latitude â†’ Latitude\n","   - longitude â†’ Longitude\n","   - depth â†’ Depth\n","\n","================================================================================\n","SAVING DATA\n","================================================================================\n","\n"," Data saved successfully!\n"," File: processed_2015_2025.csv\n"," Records: 1489681\n"," Columns: 28\n","\n"," FILE 4/4 COMPLETE!\n"]}],"source":["from usgs_earthquake_preprocessing import USGSEarthquakeProcessor\n","\n","print(\"=\"*80)\n","print(\"PROCESSING FILE 4/4: 2015-2025 DATA\")\n","print(\"=\"*80)\n","\n","processor4 = USGSEarthquakeProcessor()\n","processor4.load_data_from_file('data/usgs_earthquake_data_2015_2025.csv')\n","processor4.drop_columns(['dmin', 'magError', 'magNst'])\n","processor4.impute_missing_values(['nst', 'gap', 'horizontalError'], method='mean')\n","processor4.convert_time_format('time')\n","processor4.feature_engineering()\n","processor4.remove_duplicates()\n","processor4.standardize_columns()\n","processor4.save_processed_data('processed_2015_2025.csv')\n","\n","print(\"\\n FILE 4/4 COMPLETE!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":144434,"status":"ok","timestamp":1765204142481,"user":{"displayName":"Minh LuÃ¢n","userId":"15523114585941358677"},"user_tz":-420},"id":"H1Q36L7TWTnV","outputId":"9ff35b1d-4ff4-4e4d-bee2-ca28b2545a1d"},"outputs":[{"name":"stdout","output_type":"stream","text":["================================================================================\n"," MERGING PROCESSED EARTHQUAKE DATA FILES\n","================================================================================\n","\n"," STEP 1: Loading processed files...\n","--------------------------------------------------------------------------------\n","\n","[1/4] Loading: processed_2002_2011.csv\n","    Loaded 1,026,123 records\n","    Columns: 28\n","    Date range: 2002-01-01 00:25:29.220000+00:00 to 2011-04-13 23:58:07.650000+00:00\n","\n","[2/4] Loading: processed_2011_2013.csv\n","    Loaded 257,916 records\n","    Columns: 28\n","    Date range: 2011-04-14 00:05:44.766000+00:00 to 2013-08-30 23:58:39.575000+00:00\n","\n","[3/4] Loading: processed_2013_2015.csv\n","    Loaded 265,725 records\n","    Columns: 28\n","    Date range: 2013-08-31 00:02:26.723000+00:00 to 2015-08-28 23:48:40.730000+00:00\n","\n","[4/4] Loading: processed_2015_2025.csv\n","    Loaded 1,489,681 records\n","    Columns: 28\n","    Date range: 2015-08-01 00:07:41.692000+00:00 to 2025-12-06 09:40:47.601000+00:00\n","\n","================================================================================\n"," STEP 2: Verifying column compatibility...\n","--------------------------------------------------------------------------------\n","\n"," All files have compatible columns!\n","\n","================================================================================\n"," STEP 3: Merging dataframes...\n","--------------------------------------------------------------------------------\n","\n"," Merged successfully!\n","   Total records: 3,039,445\n","   Total columns: 28\n","\n","================================================================================\n"," STEP 4: Checking for duplicates...\n","--------------------------------------------------------------------------------\n","\n","   Initial records: 3,039,445\n","   After removing duplicates: 3,035,219\n","   Duplicates removed: 4,226\n","\n"," Found 4,226 duplicate records (likely from overlapping date ranges)\n","   These have been removed from the merged file.\n","\n","================================================================================\n"," STEP 5: Sorting by timestamp...\n","--------------------------------------------------------------------------------\n","\n"," Sorted by Timestamp\n","   Date range: 2002-01-01 00:25:29.220000+00:00 to 2025-12-06 09:40:47.601000+00:00\n","\n","================================================================================\n"," STEP 6: Merged data statistics...\n","--------------------------------------------------------------------------------\n","\n"," Magnitude Statistics:\n","   Min: -9.99\n","   Max: 9.10\n","   Mean: 1.70\n","   Median: 1.40\n","\n"," Magnitude Distribution:\n","   Great       :       26 ( 0.00%)\n","   Major       :      323 ( 0.01%)\n","   Moderate    :   39,105 ( 1.29%)\n","   Small       : 2,957,101 (97.43%)\n","   Strong      :    3,146 ( 0.10%)\n","   Unknown     :   35,518 ( 1.17%)\n","\n"," Depth Distribution:\n","   Deep        :   24,651 ( 0.81%)\n","   Intermediate:  250,284 ( 8.25%)\n","   Shallow     : 2,760,282 (90.94%)\n","   Unknown     :        2 ( 0.00%)\n","\n"," Yearly Distribution:\n","   2002: 100,183 earthquakes\n","   2003: 104,211 earthquakes\n","   2004: 123,112 earthquakes\n","   2005: 113,521 earthquakes\n","   2006: 105,718 earthquakes\n","   ... (19 more years)\n","   2021: 163,485 earthquakes\n","   2022: 150,696 earthquakes\n","   2023: 151,560 earthquakes\n","   2024: 155,940 earthquakes\n","   2025: 126,808 earthquakes\n","\n","================================================================================\n"," STEP 7: Saving merged file...\n","--------------------------------------------------------------------------------\n","\n"," Merged file saved successfully!\n","   File: merged_earthquake_data_2002_2025.csv\n","   Records: 3,035,219\n","   Columns: 28\n","   Size: 728.57 MB\n","\n","================================================================================\n"," MERGE COMPLETE!\n","================================================================================\n","\n"," Summary:\n","   Input files: 4\n","   Successfully loaded: 4\n","   Total records (before dedup): 3,039,445\n","   Total records (after dedup): 3,035,219\n","   Duplicates removed: 4,226\n","   Output file: merged_earthquake_data_2002_2025.csv\n","   File size: 728.57 MB\n","================================================================================\n","\n"]}],"source":["\"\"\"\n","Merge All Processed Earthquake Data Files\n","Combines the 4 processed CSV files into one complete dataset\n","\"\"\"\n","\n","import pandas as pd\n","import os\n","\n","def merge_processed_files():\n","    \"\"\"\n","    Merge all processed earthquake data files into one\n","    \"\"\"\n","    print(\"=\"*80)\n","    print(\" MERGING PROCESSED EARTHQUAKE DATA FILES\")\n","    print(\"=\"*80)\n","\n","    # Define the processed files to merge\n","    processed_files = [\n","        'processed_2002_2011.csv',\n","        'processed_2011_2013.csv',\n","        'processed_2013_2015.csv',\n","        'processed_2015_2025.csv'\n","    ]\n","\n","    # List to store all dataframes\n","    all_dataframes = []\n","\n","    print(\"\\n STEP 1: Loading processed files...\")\n","    print(\"-\" * 80)\n","\n","    # Load each file\n","    for i, filename in enumerate(processed_files, 1):\n","        if os.path.exists(filename):\n","            print(f\"\\n[{i}/4] Loading: {filename}\")\n","            try:\n","                df = pd.read_csv(filename)\n","                print(f\"    Loaded {len(df):,} records\")\n","                print(f\"    Columns: {len(df.columns)}\")\n","\n","                # Show date range (use flexible parsing)\n","                if 'Timestamp' in df.columns:\n","                    try:\n","                        df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='mixed', utc=True)\n","                        print(f\"    Date range: {df['Timestamp'].min()} to {df['Timestamp'].max()}\")\n","                    except:\n","                        print(f\"    Could not parse timestamps for date range\")\n","\n","                all_dataframes.append(df)\n","            except Exception as e:\n","                print(f\"    Error loading file: {str(e)}\")\n","        else:\n","            print(f\"\\n[{i}/4]  File not found: {filename}\")\n","\n","    if not all_dataframes:\n","        print(\"\\n No files were loaded. Cannot merge.\")\n","        return\n","\n","    # Check if all files have the same columns\n","    print(\"\\n\" + \"=\"*80)\n","    print(\" STEP 2: Verifying column compatibility...\")\n","    print(\"-\" * 80)\n","\n","    first_columns = set(all_dataframes[0].columns)\n","    all_compatible = True\n","\n","    for i, df in enumerate(all_dataframes[1:], start=2):\n","        current_columns = set(df.columns)\n","        if current_columns != first_columns:\n","            print(f\"\\n File {i} has different columns:\")\n","            missing = first_columns - current_columns\n","            extra = current_columns - first_columns\n","            if missing:\n","                print(f\"   Missing: {missing}\")\n","            if extra:\n","                print(f\"   Extra: {extra}\")\n","            all_compatible = False\n","\n","    if all_compatible:\n","        print(\"\\n All files have compatible columns!\")\n","    else:\n","        print(\"\\n Warning: Files have different columns. Proceeding with merge...\")\n","\n","    # Merge all dataframes\n","    print(\"\\n\" + \"=\"*80)\n","    print(\" STEP 3: Merging dataframes...\")\n","    print(\"-\" * 80)\n","\n","    merged_df = pd.concat(all_dataframes, ignore_index=True)\n","    print(f\"\\n Merged successfully!\")\n","    print(f\"   Total records: {len(merged_df):,}\")\n","    print(f\"   Total columns: {len(merged_df.columns)}\")\n","\n","    # Check for overlapping data\n","    print(\"\\n\" + \"=\"*80)\n","    print(\" STEP 4: Checking for duplicates...\")\n","    print(\"-\" * 80)\n","\n","    initial_count = len(merged_df)\n","    merged_df = merged_df.drop_duplicates()\n","    final_count = len(merged_df)\n","    duplicates = initial_count - final_count\n","\n","    print(f\"\\n   Initial records: {initial_count:,}\")\n","    print(f\"   After removing duplicates: {final_count:,}\")\n","    print(f\"   Duplicates removed: {duplicates:,}\")\n","\n","    if duplicates \u003e 0:\n","        print(f\"\\n Found {duplicates:,} duplicate records (likely from overlapping date ranges)\")\n","        print(\"   These have been removed from the merged file.\")\n","    else:\n","        print(\"\\n No duplicate records found!\")\n","\n","    # Sort by timestamp\n","    print(\"\\n\" + \"=\"*80)\n","    print(\" STEP 5: Sorting by timestamp...\")\n","    print(\"-\" * 80)\n","\n","    if 'Timestamp' in merged_df.columns:\n","        try:\n","            # Ensure timestamp is in datetime format\n","            if merged_df['Timestamp'].dtype == 'object':\n","                merged_df['Timestamp'] = pd.to_datetime(merged_df['Timestamp'], format='mixed', utc=True)\n","\n","            merged_df = merged_df.sort_values('Timestamp').reset_index(drop=True)\n","            print(f\"\\n Sorted by Timestamp\")\n","            print(f\"   Date range: {merged_df['Timestamp'].min()} to {merged_df['Timestamp'].max()}\")\n","        except Exception as e:\n","            print(f\"\\n Could not sort by timestamp: {str(e)}\")\n","\n","    # Display statistics\n","    print(\"\\n\" + \"=\"*80)\n","    print(\" STEP 6: Merged data statistics...\")\n","    print(\"-\" * 80)\n","\n","    if 'Magnitude' in merged_df.columns:\n","        print(f\"\\n Magnitude Statistics:\")\n","        print(f\"   Min: {merged_df['Magnitude'].min():.2f}\")\n","        print(f\"   Max: {merged_df['Magnitude'].max():.2f}\")\n","        print(f\"   Mean: {merged_df['Magnitude'].mean():.2f}\")\n","        print(f\"   Median: {merged_df['Magnitude'].median():.2f}\")\n","\n","    if 'magnitude_category' in merged_df.columns:\n","        print(f\"\\n Magnitude Distribution:\")\n","        mag_dist = merged_df['magnitude_category'].value_counts().sort_index()\n","        for category, count in mag_dist.items():\n","            pct = (count / len(merged_df) * 100)\n","            print(f\"   {category:12s}: {count:8,} ({pct:5.2f}%)\")\n","\n","    if 'depth_category' in merged_df.columns:\n","        print(f\"\\n Depth Distribution:\")\n","        depth_dist = merged_df['depth_category'].value_counts().sort_index()\n","        for category, count in depth_dist.items():\n","            pct = (count / len(merged_df) * 100)\n","            print(f\"   {category:12s}: {count:8,} ({pct:5.2f}%)\")\n","\n","    if 'year' in merged_df.columns:\n","        print(f\"\\n Yearly Distribution:\")\n","        yearly_dist = merged_df['year'].value_counts().sort_index()\n","        for year, count in yearly_dist.head(5).items():\n","            print(f\"   {year}: {count:7,} earthquakes\")\n","        if len(yearly_dist) \u003e 5:\n","            print(f\"   ... ({len(yearly_dist) - 5} more years)\")\n","            for year, count in yearly_dist.tail(5).items():\n","                print(f\"   {year}: {count:7,} earthquakes\")\n","\n","    # Save merged file\n","    print(\"\\n\" + \"=\"*80)\n","    print(\" STEP 7: Saving merged file...\")\n","    print(\"-\" * 80)\n","\n","    output_file = 'merged_earthquake_data_2002_2025.csv'\n","\n","    try:\n","        merged_df.to_csv(output_file, index=False)\n","\n","        # Get file size\n","        file_size = os.path.getsize(output_file)\n","        size_mb = file_size / (1024 * 1024)\n","\n","        print(f\"\\n Merged file saved successfully!\")\n","        print(f\"   File: {output_file}\")\n","        print(f\"   Records: {len(merged_df):,}\")\n","        print(f\"   Columns: {len(merged_df.columns)}\")\n","        print(f\"   Size: {size_mb:.2f} MB\")\n","\n","    except Exception as e:\n","        print(f\"\\n Error saving file: {str(e)}\")\n","        return\n","\n","    # Final summary\n","    print(\"\\n\" + \"=\"*80)\n","    print(\" MERGE COMPLETE!\")\n","    print(\"=\"*80)\n","\n","    print(f\"\\n Summary:\")\n","    print(f\"   Input files: {len(processed_files)}\")\n","    print(f\"   Successfully loaded: {len(all_dataframes)}\")\n","    print(f\"   Total records (before dedup): {initial_count:,}\")\n","    print(f\"   Total records (after dedup): {final_count:,}\")\n","    print(f\"   Duplicates removed: {duplicates:,}\")\n","    print(f\"   Output file: {output_file}\")\n","    print(f\"   File size: {size_mb:.2f} MB\")\n","    print(\"=\"*80 + \"\\n\")\n","\n","if __name__ == \"__main__\":\n","    merge_processed_files()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19,"status":"ok","timestamp":1765704025605,"user":{"displayName":"Minh LuÃ¢n","userId":"15523114585941358677"},"user_tz":-420},"id":"tRmTYbeoJEZB","outputId":"17127747-b518-4b8a-8c0b-97e8b4739be2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading merged earthquake data...\n","Error: The file 'merged_earthquake_data_2002_2025.csv' was not found.\n","Please ensure that the previous cell (merging processed files) was run successfully.\n","You might need to re-run the cell that generates 'merged_earthquake_data_2002_2025.csv'.\n"]}],"source":["import pandas as pd\n","import os # Import os module to check file existence\n","\n","file_path = 'merged_earthquake_data_2002_2025.csv'\n","\n","print(\"Loading merged earthquake data...\")\n","\n","if not os.path.exists(file_path):\n","    print(f\"Error: The file '{file_path}' was not found.\")\n","    print(\"Please ensure that the previous cell (merging processed files) was run successfully.\")\n","    print(\"You might need to re-run the cell that generates 'merged_earthquake_data_2002_2025.csv'.\")\n","else:\n","    df = pd.read_csv(file_path)\n","\n","    print(f\"Loaded {len(df):,} records with {len(df.columns)} columns\\n\")\n","\n","    print(\"=\"*80)\n","    print(\"MISSING DATA ANALYSIS\")\n","    print(\"=\"*80)\n","\n","    missing_count = df.isnull().sum()\n","    missing_percent = (missing_count / len(df) * 100).round(2)\n","\n","    missing_df = pd.DataFrame({\n","        'Column': missing_count.index,\n","        'Missing Count': missing_count.values,\n","        'Missing %': missing_percent.values\n","    })\n","\n","    missing_df = missing_df.sort_values('Missing %', ascending=False)\n","\n","    print(\"\\nColumns with Missing Data:\\n\")\n","    cols_with_missing = missing_df[missing_df['Missing Count'] \u003e 0]\n","    print(cols_with_missing.to_string(index=False))\n","\n","    total_cells = len(df) * len(df.columns)\n","    total_missing = missing_count.sum()\n","    total_missing_pct = (total_missing / total_cells * 100).round(2)\n","\n","    print(\"\\n\" + \"=\"*80)\n","    print(\"OVERALL STATISTICS\")\n","    print(\"=\"*80)\n","    print(f\"Total rows: {len(df):,}\")\n","    print(f\"Total columns: {len(df.columns)}\")\n","    print(f\"Total cells: {total_cells:,}\")\n","    print(f\"Total missing: {total_missing:,} ({total_missing_pct}%)\")\n","    print(f\"Data completeness: {100 - total_missing_pct:.2f}%\")\n","    print(f\"\\nColumns with missing data: {len(cols_with_missing)}/{len(df.columns)}\")\n","    print(f\"Columns with NO missing data: {len(df.columns) - len(cols_with_missing)}/{len(df.columns)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1765704072221,"user":{"displayName":"Minh LuÃ¢n","userId":"15523114585941358677"},"user_tz":-420},"id":"c7e5f048","outputId":"df514357-9a65-429d-888e-1fa973c9463a"},"outputs":[{"name":"stdout","output_type":"stream","text":["File 'merged_earthquake_data_2002_2025.csv' does NOT exist.\n","Please ensure the merge process in cell H1Q36L7TWTnV was completed successfully.\n"]}],"source":["import os\n","\n","file_name = 'merged_earthquake_data_2002_2025.csv'\n","\n","if os.path.exists(file_name):\n","    print(f\"File '{file_name}' exists.\")\n","    file_size_bytes = os.path.getsize(file_name)\n","    file_size_mb = file_size_bytes / (1024 * 1024)\n","    print(f\"File size: {file_size_mb:.2f} MB\")\n","else:\n","    print(f\"File '{file_name}' does NOT exist.\")\n","    print(\"Please ensure the merge process in cell H1Q36L7TWTnV was completed successfully.\")"]}],"metadata":{"colab":{"name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}